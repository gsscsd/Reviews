### 1、L1正则化

L1正则化算法用来防止过拟合时，是在损失函数上加入$|w|$，如下式所示：

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdn.net/20181016093055645?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5NDYyODQ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

在优化损失函数的时候L1正则化会产生稀疏矩阵，导致一部分w为0，注意这也是L1正则化的核心思想。产生稀疏矩阵之后，一部分w为0，一部分不为0，这样即可对特征进行选择。选择比较重要、明显的特征作为分类和预测的依据，抛弃那些不重要的特征。

### 2、L2正则化

L2正则化算法用来防止过拟合时，是在算是函数上加上$|w|^2$ ,如下式所示：

![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdn.net/20181016093540524?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI5NDYyODQ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


不同于L1正则化，L2正则化则是趋向于把所有参数w都变得比较小，一般认为参数w比较小的时候，模型比较简单。直观上来说，L2正则化的解都比较小，抗扰动能力强。在求解过程中，L2通常倾向让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。参数足够小，数据偏移得多一点也不会对结果造成什么影响，可以说“抗扰动能力强”。

### 3、BN算法

Batch Normalization有两个功能，一个是可以加快训练和收敛速度，另外一个是可以防止过拟合。

BN算法是如何加快训练和收敛速度的呢？

> BN算法在实际使用的时候会把特征给强制性的归到均值为0，方差为1的数学模型下。深度网络在训练的过程中，如果每层的数据分布都不一样的话，将会导致网络非常难收敛和训练，而如果能把每层的数据转换到均值为0，方差为1的状态下，一方面，数据的分布是相同的，训练会比较容易收敛，另一方面，均值为0，方差为1的状态下，在梯度计算时会产生比较大的梯度值，可以加快参数的训练，更直观的来说，是把数据从饱和区直接拉到非饱和区。更进一步，这也可以很好的控制梯度爆炸和梯度消失现象，因为这两种现象都和梯度有关。

**BN最大的优点为允许网络使用较大的学习速率进行训练，加快网络的训练速度。**

BN算法时如何防止过拟合的？

在这里摘录一段国外大神的解释：

When training with Batch Normalization, a training example is seen in conjunction with other examples in the mini-batch, and the training network no longer producing deterministic values for a given training example. In our experiments, we found this effect to be advantageous to the generalization of the network.

**大概意思是：在训练中，BN的使用使得一个mini-batch中的所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果。**

**这句话什么意思呢？意思就是同样一个样本的输出不再仅仅取决于样本本身，也取决于跟这个样本属于同一个mini-batch的其它样本。同一个样本跟不同的样本组成一个mini-batch，它们的输出是不同的（仅限于训练阶段，在inference阶段是没有这种情况的）。我把这个理解成一种数据增强：同样一个样本在超平面上被拉扯，每次拉扯的方向的大小均有不同。不同于数据增强的是，这种拉扯是贯穿数据流过神经网络的整个过程的，意味着神经网络每一层的输入都被数据增强处理了。**

相比于Dropout、L1、L2正则化来说，BN算法防止过拟合效果没那末明显。

### 4、Dropout算法

Dropout为什么能够防止过拟合呢？

最直观的原因其实就是：防止参数过分依赖训练数据，增加参数对数据集的泛化能力。因为在实际训练的时候，每个参数都有可能被随机的Drop掉，所以参数不会过分的依赖某一个特征的数据，而且不同参数之间的相互关联性也大大减弱，这些操作都可以增加泛化能力。

更为深入的来讲，Dropout其实是一种分布式表示：

分布式表征（Distributed Representation），是人工神经网络研究的一个核心思想。那什么是分布式表征呢？简单来说，就是当我们表达一个概念时，神经元和概念之间不是一对一对应映射（map）存储的，它们之间的关系是多对多。具体而言，就是一个概念可以用多个神经元共同定义表达，同时一个神经元也可以参与多个不同概念的表达，只不过所占的权重不同罢了。

举例来说，对于“小红汽车”这个概念，如果用分布式特征地表达，那么就可能是一个神经元代表大小（形状：小），一个神经元代表颜色（颜色：红），还有一个神经元代表车的类别（类别：汽车）。只有当这三个神经元同时被激活时，就可以比较准确地描述我们要表达的物体。

分布式表征表示有很多优点。其中最重要的一点，莫过于当部分神经元发生故障时，信息的表达不会出现覆灭性的破坏。比如，我们常在影视作品中看到这样的场景，仇人相见分外眼红，一人（A）发狠地说，“你化成灰，我都认识你（B）！”这里并不是说B真的“化成灰”了，而是说，虽然时过境迁，物是人非，当事人B外表也变了很多（对于识别人A来说，B在其大脑中的信息存储是残缺的），但没有关系，只要B的部分核心特征还在，那A还是能够把B认得清清楚楚、真真切切！人类的大脑还是真的厉害啊！

再借用某大牛博主的一段话：

**在学习阶段，以概率p主动临时性地忽略掉部分隐藏节点。这一操作的好处在于，在较大程度上减小了网络的大小，而在这个“残缺”的网络中，让神经网络学习数据中的局部特征（即部分分布式特征）。在多个“残缺”之网（相当于多个简单网络）中实施特征，总要比仅在单个健全网络上进行特征学习，其泛化能力来得更加健壮。这里的“泛化”，实际上就是适应各种情况的能力。如果神经网络仅仅在训练集合上表现好（好比“窝里横”），而在应对其他新情况表现不佳，就表明陷入“过拟合（Overfitting）”状态，其实就是泛化能力差。**

经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。

