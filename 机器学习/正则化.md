## 正则化

正则化也是校招中常考的题目之一，在去年的校招中，被问到了多次：

**1、过拟合的解决方式有哪些，l1和l2正则化都有哪些不同，各自有什么优缺点(爱奇艺)**

**2、L1和L2正则化来避免过拟合是大家都知道的事情，而且我们都知道L1正则化可以得到稀疏解，L2正则化可以得到平滑解，这是为什么呢？**

**3、L1和L2有什么区别，从数学角度解释L2为什么能提升模型的泛化能力。（美团）**

**4、L1和L2的区别，以及各自的使用场景（头条）**

接下来，咱们就针对上面的几个问题，进行针对性回答！

### 1、什么是L1正则&L2正则？

L1正则即将参数的绝对值之和加入到损失函数中，以二元线性回归为例，损失函数变为：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLnCzZzSYnvkvyCAaSibFTXOsqGSvbRQHcDHUAzvtkt7HsxWjRicPvotOw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

L2正则即将参数的平方之和加入到损失函数中，以二元线性回归为例，损失函数变为：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsL59ibkRCOtzRDFPV4KrT1LwtuM6Ev8Ttcj4Fs4472RcSdDw3metAibeRg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 2、L1正则&L2正则的区别是什么？

二者的区别的话，咱们总结主要有以下两点，最主要的还是第二点：

1、L1正则化是指在损失函数中加入权值向量w的绝对值之和，即各个元素的绝对值之和，L2正则化指在损失函数中加入权值向量w的平方和。

2、L1的功能是使权重稀疏，而L2的功能是使权重平滑。

### 3、L1正则为什么可以得到稀疏解？

这一道题是面试中最容易考到的，大家一定要理解掌握！这一部分的回答，在《百面机器学习》中给出了三种答案：

#### 3.1 解空间形状

这是我们最常使用的一种答案，就是给面试官画如下的图：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLGIHtQ4wiazqXwhe1ekHVYUFeCeuib9qlYibe1ypfnQLnrRyN3hyZ85PlQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

L2正则化相当于为参数定义了一个圆形的解空间，而L1正则化相当于为参数定义了一个菱形的解空间。L1“棱角分明”的解空间显然更容易与目标函数等高线在脚点碰撞。从而产生稀疏解。

#### 3.2 函数叠加

我们考虑一维的情况，横轴是参数的值，纵轴是损失函数，加入正则项之后，损失函数曲线图变化如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLLksROKo8l0KrCVGfdf7eiabK2QKm94kPFc5VtLiaM9hNFr3ZFjbXyUQg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到，在加入L1正则项后，最小值在红点处，对应的w是0。而加入L2正则项后，最小值在黄点处，对应的w并不为0。

为什么呢？加入L1正则项后，目标函数变为L(w)+C|w|，单就正则项部分求导，原点左边的值为-C，原点右边的值为C，因此，只要原目标函数的导数绝对值|L'(w)|<C,那么带L1正则项的目标函数在原点左边部分始终递减，在原点右边部分始终递增，最小值点自然会出现在原点处。

加入L2正则项后，目标函数变为L(w)+Cw2，只要原目标函数在原点处的导数不为0，那么带L2正则项的目标函数在原点处的导数就不为0，那么最小值就不会在原点。因此L2正则只有见效w绝对值的作用，但并不能产生稀疏解。

#### 3.3 贝叶斯先验

从贝叶斯角度来看，L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验(为什么我们在后面详细解释)。我们来看一下高斯分布和拉普拉斯分布的形状：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLYFMjV0hkkS1oTRw8haRBZRu8F6xb5SphtdFHUWL2CRO2rsCL3GbkaA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLVvsJKLFT5HhSy1N0AwiabQCbdgn4guiaRhzLI8oy4qGk5aBJQYjKyCzw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到，当均值为0时，高斯分布在极值点处是平滑的，也就是高斯先验分布认为w在极值点附近取不同值的可能性是接近的。但对拉普拉斯分布来说，其极值点处是一个尖峰，所以拉普拉斯先验分布中参数w取值为0的可能性要更高。

### 4、从数学角度解释L2为什么能提升模型的泛化能力

这里主要给出两篇博客作为参考：
https://www.zhihu.com/question/35508851
https://blog.csdn.net/zouxy09/article/details/24971995

### 5、为什么说“L1正则化相当于对模型参数w引入了拉普拉斯先验，L2正则化相当于引入了高斯先验”？

这一部分咱们小小推导一下，嘻嘻，如果一看数学就头大的同学，可以跳过此处。

在贝叶斯估计中，我们要求解的是参数θ的后验概率最大化：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLyFP1ib2zxU5jgluDNgC5Ub4BINia0WhvUqibW2WXD5rp58CDGYhdfxOSQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在最后一项的分子中P(Xi|θ)和分母都是一个常数，因此，上式可以继续化简：

所以贝叶斯学派估计是使下面的式子最小化：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLWC76fQ8X0qoU0WZOEbKsW7qcuibm2VhHmxWKLj6LqJRS3hiaPiaRWfEnA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

关于第一项，假设我们做的是一元线性回归，那么求解过程如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLBWvCZ43LuYYZjJ0AILrfLLpmS0e9GfVULLXDThrv5NH8cJdMscDQPg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

第二项，咱们就得分类讨论了，如果θ服从的是0均值的高斯分布，为了和上面的方差所区分，这里咱们用alpha来表示，那么有：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsL12AyCQzVett3lKx84mP8icuiciaIKfbELMCtCcYZx0ndQkGEFibPicEibcaw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

所以，最终可以得到：

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLriaZ1QftI85aOCvsVkcxyF4W9eELeSt27WuicvtCYBH5OWQezkxyJGfw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们把与θ无关的情况去掉，便得到：

你可能觉得，alpha不是θ的方差么，请注意，这里是先验分布，我们可以任意指定alpha的值，所以去掉也是可以的。

同理，我们可以得到当先验是拉普拉斯分布时的情况。

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLBicn29DGp1rGLLX5O23tPk2F5WNSTENHQiciaaaE6ZXwpoxqtXAP0B0Tg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/jYWFficmyzX6mUom85CDufMiayB6ACQUsLJVZuE7ljrpGbBJkQ30GIFFzicW6Qv9cC5psNCACSUxDKic3ZLQItsUKA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)