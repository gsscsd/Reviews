## L1、L2正则化来源推导

L1L2的推导可以从两个角度：

- 带约束条件的优化求解（拉格朗日乘子法）
- 贝叶斯学派的：最大后验概率

### 1.1 基于约束条件的最优化

对于模型权重系数w的求解释通过最小化目标函数实现的，也就是求解：

![img](https://img-blog.csdn.net/20180726075446376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

首先，模型的复杂度可以用VC来衡量。通常情况下，模型VC维与系数w的个数成线性关系：即：

**w数量越多，VC越大，模型越复杂**

为了限制模型的复杂度，我们要降低VC，自然的思路就是降低w的数量，即：

让w向量中的一些元素为0或者说限制w中非零元素的个数。我们可以在原优化问题上加入一些优化条件：

![img](https://img-blog.csdn.net/20180726075746164?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**其中约束条件中的||w||0是指L0范数，表示的是向量w中非零元素的个数，让非零元素的个数小于某一个C，就能有效地控制模型中的非零元素的个数，但是这是一个NP问题，不好解，于是我们需要做一定的“松弛”。为了达到我们想要的效果（权重向量w中尽可能少的非零项），我们不再严格要求某些权重w为0，而是要求权重w向量中某些维度的非零参数尽可能接近于0，尽可能的小，这里我们可以使用L1L2范数来代替L0范数**，即：

![img](https://img-blog.csdn.net/20180726080224863?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

注意哈：这里使用L2范数的时候，为了后续处理（其实就是为了优化），可以对进行平方，只需要调整C的取值即可。

然后我们利用拉式乘子法求解：

![img](https://img-blog.csdn.net/20180726080508988?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其中这里的是拉格朗日系数，$\alpha$>0，我们假设的$\alpha$最优解为$\alpha*$，对拉格朗日函数求最小化等价于：

![img](https://img-blog.csdn.net/20180726080700716?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上面和

![img](https://img-blog.csdn.net/20180726080731366?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

等价。所以我们这里得到对L1L2正则化的第一种理解：

**L1正则化   在原优化目标函数中增加约束条件**

**L2正则化  在原优化目标函数中增加约束条件**

### 1.2基于最大后验概率估计

在最大似然估计中，是假设权重w是未知的参数，从而求得对数似然函数（取了log）：

![img](https://img-blog.csdn.net/20180726081420159?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

从上式子可以看出：假设的$y^i$不同概率分布，就可以得到不同的模型。

若我们假设：

![img](https://img-blog.csdn.net/20180726081643289?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

的高斯分布，我们就可以带入高斯分布的概率密度函数：

![img](https://img-blog.csdn.net/20180726081805799?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

上面的C为常数项，常数项和系数不影响我们求解的解，所以我们可以令

![img](https://img-blog.csdn.net/20180726082023410?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

我们就得到了Linear Regursion的代价函数。

在最大化后验概率估计中，我们将权重w看做随机变量，也具有某种分布，从而有：

![img](https://img-blog.csdn.net/201807260821493?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

同样取对数：

![img](https://img-blog.csdn.net/20180726082212162?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

可以看出来后验概率函数为在似然函数的基础上增加了logP(w)，P（w）的意义是对权重系数w的概率分布的先验假设，在收集到训练样本{X，y}后，则可根据w在{X，y}下的后验概率对w进行修正，从而做出对w的更好地估计。

若假设的先验分布为0均值的高斯分布，即

![img](https://img-blog.csdn.net/201807260841100?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

则有：

![img](https://img-blog.csdn.net/20180726084127445?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

可以看到，在高斯分布下,$logP(w)$的效果等价于在代价函数中增加L2正则项。

若假设$w_j$服从均值为0，参数为a的拉普拉斯分布，即：

![img](https://img-blog.csdn.net/20180726084423322?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

则有：

![img](https://img-blog.csdn.net/20180726084442764?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yX1hpYW9a/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

可以看到，在拉普拉斯分布下$logP(w)$的效果等价在代价函数中增加L1正项。

故此，我们得到对于L1，L2正则化的第二种理解：

> L1正则化可通过假设权重w的先验分布为拉普拉斯分布，由最大后验概率估计导出。
>
> L2正则化可通过假设权重w的先验分布为高斯分布，由最大后验概率估计导出。

