## **（1）为什么将LR和SVM放在一起来进行比较？**

回答这个问题其实就是回答LR和SVM有什么相同点。

**第一，LR和SVM都是分类算法。**

看到这里很多人就不会认同了，因为在很大一部分人眼里，LR是回归算法。我是非常不赞同这一点的，因为我认为判断一个算法是分类还是回归算法的唯一标准就是样本label的类型，如果label是离散的，就是分类算法，如果label是连续的，就是回归算法。很明显，LR的训练数据的label是“0或者1”，当然是分类算法。其实这样不重要啦，暂且迁就我认为他是分类算法吧，再说了，SVM也可以回归用呢。

**第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。**

这里要先说明一点，那就是LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？

**第三，LR和SVM都是监督学习算法。**

这个就不赘述什么是监督学习，什么是半监督学习，什么是非监督学习了。

**第四，LR和SVM都是判别模型。**

判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别（哈哈，废话是不是太多）。

**第五，LR和SVM在学术界和工业界都广为人知并且应用广泛。**

讲完了LR和SVM的相同点，你是不是也认为有必要将他们进行比较一下了呢？而且比较LR和SVM，是不是比让你比较决策树和LR、决策树和SVM、朴素贝叶斯和LR、朴素贝叶斯和SVM更能考察你的功底呢？

## **（2）LR和SVM的不同。**

**第一，本质上是其loss function不同。**

[![img](http://s10.sinaimg.cn/mw690/002n6ruKgy6WWsUQfxf29)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002n6ruKgy6WWsUQfxf29)逻辑回归的损失函数

[![img](http://s4.sinaimg.cn/mw690/002n6ruKgy6WWtjCmm793)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002n6ruKgy6WWtjCmm793)支持向量机的目标函数

不同的loss function代表了不同的假设前提，也就代表了不同的分类原理，也就代表了一切！！！简单来说，逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值，具体细节参考<http://blog.csdn.net/pakko/article/details/37878837>。支持向量机基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面，具体细节参考<http://blog.csdn.net/macyang/article/details/38782399>

**第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。**

当你读完上面两个网址的内容，深入了解了LR和SVM的原理过后，会发现影响SVM决策面的样本点只有少数的结构支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果。用下图进行说明：

[![img](http://s1.sinaimg.cn/mw690/002n6ruKgy6WWvMHbGgb0)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002n6ruKgy6WWvMHbGgb0)支持向量机改变非支持向量样本并不会引起决策面的变化

[![img](http://s5.sinaimg.cn/mw690/002n6ruKgy6WWw74KqM04)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002n6ruKgy6WWw74KqM04)逻辑回归中改变任何样本都会引起决策面的变化

理解了这一点，有可能你会问，然后呢？有什么用呢？有什么意义吗？对使用两种算法有什么帮助么？一句话回答：

因为上面的原因，得知：线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。（引自<http://www.zhihu.com/question/26768865/answer/34078149>）

**第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。**

这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。

**第四，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。**（引自<http://www.zhihu.com/question/26768865/answer/34078149>）

一个机遇概率，一个机遇距离！

**第五，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！**

以前一直不理解为什么SVM叫做结构风险最小化算法，所谓结构风险最小化，意思就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，从而达到真实误差的最小化。未达到结构风险最小化的目的，最常用的方法就是添加正则项，后面的博客我会具体分析各种正则因子的不同，这里就不扯远了。但是，你发现没，SVM的目标函数里居然自带正则项！！！再看一下上面提到过的SVM目标函数：

[![img](http://s9.sinaimg.cn/mw690/002n6ruKgy6WWxdRoxy08)](http://photo.blog.sina.com.cn/showpic.html#blogid=&url=http://album.sina.com.cn/pic/002n6ruKgy6WWxdRoxy08)SVM目标函数

有木有，那不就是L2正则项吗？