## 包含L1正则化项的目标函数如何求导

### 1. 用坐标轴下降法求解Lasso回归

　　　　坐标轴下降法顾名思义，是沿着坐标轴的方向去下降，这和梯度下降不同。梯度下降是沿着梯度的负方向下降。不过梯度下降和坐标轴下降的共性就都是迭代法，通过启发式的方式一步步迭代求解函数的最小值。

　　　　坐标轴下降法的数学依据主要是这个结论（此处不做证明）：一个可微的凸函数J(θ) 其中θ是nx1的向量，即有n个维度。如果在某一点$\theta$，使得J(θ)在每一个坐标轴上$\overline \theta (i=1,2,...)$都是最小值，那么$J(\overline \theta_i)$就是一个全局的最小值。

　　　于是我们的优化目标就是在θ的n个坐标轴上(或者说向量的方向上)对损失函数做迭代的下降，当所有的坐标轴上的$θ_i(i = 1,2,...n)$都达到收敛时，我们的损失函数最小，此时的θ即为我们要求的结果。

　		坐标轴下降法可以和梯度下降做一个比较：

　　　　a) 坐标轴下降法在每次迭代中在当前点处沿一个坐标方向进行一维搜索 ，固定其他的坐标方向，找到一个函数的局部极小值。而梯度下降总是沿着梯度的负方向求函数的局部最小值。

　　　　b) 坐标轴下降优化方法是一种非梯度优化算法。在整个过程中依次循环使用不同的坐标方向进行迭代，一个周期的一维搜索迭代过程相当于一个梯度下降的迭代。

　　　　c) 梯度下降是利用目标函数的导数来确定搜索方向的，该梯度方向可能不与任何坐标轴平行。而坐标轴下降法法是利用当前坐标方向进行搜索，不需要求目标函数的导数，只按照某一坐标方向进行搜索最小值。

　　　　d) 两者都是迭代方法，且每一轮迭代，都需要O(mn)的计算量(m为样本数，n为系数向量的维度)

### 2.近端梯度下降法（Proximal Gradient Decent ）

![img](https://img-blog.csdn.net/20170513151906755)

其他方法，参见：<https://www.cnblogs.com/pinard/p/6018889.html>