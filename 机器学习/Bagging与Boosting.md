## Bagging与Boosting方法对比

## 1、bagging的原理

Bagging：基于数据**随机重抽样**的分类器构建方法。从训练集从进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。

![img](https://img-blog.csdn.net/20180910131918659?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIwNDEyNTk1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

> **随机采样(bootsrap)**就是从我们的训练集里面采集固定个数的样本，但是**每采集一个样本后，都将样本放回**。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于我们的Bagging算法，**一般会随机采集和训练集样本数m一样个数的样本**。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有m个样本训练集做T次的随机采样，，则由于随机性，T个采样集各不相同。
>
> 注意到这和GBDT的子采样是不同的。**GBDT的子采样是无放回采样，而Bagging的子采样是放回采样。**

#### RF的改进

RF引入了**bagging和随机子空间**的思想。**随机森林是一个用算法构建的没有剪枝的分类决策树的集合**，森林的输出釆用**简单多数投票法**，或者是单棵树输出结果的简单平均得到。

具体的训练过程如下：

- 步骤1 利用bagging思想，随机产生样本子集。
- 步骤2 利用随机子空间思想，随机抽取f个特征，进行节点分裂，构建单棵回归决策子树。
- 步骤3 重复步骤1、步骤2，构建T棵回归决策子树，每棵树自由生长，**不进行剪枝，形成森林。**
- 步骤4 T棵决策子树的预测值取平均，作为最终结果

> 1. RF使用了**CART决策树作为弱学习器**，这让我们想到了梯度提升树GBDT。
> 2. 在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但**是RF通过随机选择节点上的一部分样本特征**，这个数字小于n，假设为$n_{sub}$，然后在这些随机选择的个样本特征中**，选择一个最优的特征**来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。
>
> 如果$n_{sub}=n$，则此时RF的CART决策树和普通的CART决策树没有区别。$n_{sub}$越小，则模型越健壮，当然此时对于训练集的拟合程度会变差。也就是说$n_{sub}$越小，模型的方差会减小，但是偏差会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的$n_{sub}$的值。

**袋外错误率：（OOB）**

对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\small \frac{1}{m}$。不被采集到的概率为$\small 1-\frac{1}{m}$。如果m次采样都没有被采集中的概率是$\small (1-\frac{1}{m})^{m}$。当$\small m\rightarrow \infty$时，$\small (1-\frac{1}{m})^{m}=\frac{1}{e}\approx 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。

> - 对“袋外”样本进行多棵决策树的融合预测，得到的预测值与真实值的误差率就是“袋外错误率”oob
> - 有了oob，我们发现**随机森林算法不需要单独的测试集来防止过拟合，oob就可以作为单独测试集的作用**









#### Adaboost和GBDT的区别和联系

它们都属于boosting提升方法，AdaBoost 是通过提升错分数据点的权重来定位模型的不足，而Gradient Boosting是通过计算负梯度来定位模型的不足。