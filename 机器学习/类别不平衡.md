# **类别不平衡问题的三种基础处理方案**

# 1  简介

在很多问题中,例如交易欺诈,广告点击率预测,病毒脚本判断等问题中都会出现一个非常大的问题,就是正样本特别少,负样本特别多; 那么如何处理这类的问题呢？本篇文章，我们分享三种处理技巧，当然这些技巧不是万能的，需要针对特定的问题使用！

例如：

- 如果我们对于召回有特别大的需求，也就是说每个正样本的预测都远远比一个负样本的预测要重要，那么这个时候如果不做任何处理，很多时候就没法拿到我们所希望的结果；
- 如果我们问题的指标是AUC，那么此时很多实战的朋友会发现，这个时候不处理和处理的差别没那么大，就好比于一个参数的波动，虽然将处理后的结果和不处理的结果进行融合一般都是有细微提升的。
- 如果在我们的任务中正样本和负样本是同等重要的，即预测对一个正样本和预测对一个负样本是同等重要的，那么其实不做其他的处理也影响不大。

当然很多时候我们发现,在很多常见的AUC等问题中,下面的方式一般都可以带来细微的提升.

> **Result( blending(pred过采样,pred降采样, pred不处理, pred权重) ) > pred不处理**

所以本篇文章,我们就介绍这三样技术:

1. 加权处理;
2. 过采样;
3. 降采样;





# 2  类别不平衡样本处理三招

## 2.1  加权处理

这种方法是最常见的,在很多特定的问题中,例如多分类问题的MicroFscore或者这次百度比赛的Weighted Fscore为指标的问题中,不同类的权重不一样,虽然我们可以通过后处理来提升我们结果的准确率,但是实践中我们发现,通过对不同类别进行加权的方式我们有时也可以达到相同的效果。

加权的操作很简单,步骤如下:

1. 遍历每一个样本;
2. 如果样本满足某一个要求,例如在不平衡的二分类中,如果样本的标签为1,那么我们将其权重设置为w1(自定义);如果样本标签为0;那么我们将其权重设置为w2(自定义);
3. 将样本权重代入模型进行训练&测试.

加权的直观理解就是我认为一个正样本的价值顶得上多个负样本,那么我们希望将其在模型的重要性调整为某一个权重。接下来,我们介绍两种实战中十分常见的采样策略:过采样和欠采样,这两个概念十分简单,不做过多阐述,具体参考下图:

![img](https://mmbiz.qpic.cn/mmbiz_png/k20ecIowF5ZIpca2l6vMH1qmq4UPMI4RKKRFibxRyIica9iaN5HK6Lh4sprtfO4EOgaKqeqSQvTL7tq501E4oWNDg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 2.2  过采样

过采样的技术有非常多,最常见的就是随机过采样和SMOTE过采样。随机过采样就是从少的类中进行随机进行采样然后拼接上去,这种效果很多时候和加权差不大。还有一种较常见的也是现在比赛中出现最多的采样方法,SMOTE采样。

SMOTE的示意图如下,

![img](https://mmbiz.qpic.cn/mmbiz_png/k20ecIowF5ZIpca2l6vMH1qmq4UPMI4RtyiaALz4KetoQbYOBTchbOFNWlGFdTVE6f6T1hRooia9j4RVC3zw7IoQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



SMOTE算法的生成过程为:

1. 对于少数类中每一个样本x，以欧氏距离为标准计算它到少数类样本集中所有样本的距离，得到其k近邻。
2. 根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为xn。
3. 对于每一个随机选出的近邻xn，分别与原样本按照如下的公式构建新的样本



![img](https://mmbiz.qpic.cn/mmbiz_png/k20ecIowF5ZIpca2l6vMH1qmq4UPMI4RJt6ZDOYPyF4Qy8bHy9K7Jiba1BVicZubHRPrVcMRZeDRG7lJ6OYacNvQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从算法中,我们可以发现,SMOTE采样其实就是生成样本之间的一些样本.思路很简单朴实,现在几乎每个类别不平衡的比赛都会出现SMOTE算法的背影。但是因为思路简单,我们也很容易就可以发现SMOTE算法的一些缺点。

- 在近邻选择时,K值的决定一般较难,可以枚举然后根据实验效果来定;
- 算法无法克服非平衡数据集的数据分布问题,容易产生分布边缘化问题. 如果正样本都分布在边缘,我们通过采样正样本来生成样本,那么这样新生成的样本将也会全部在边缘，且会越来越边缘化,从而模糊了正类样本和负类样本的边界,而且使边界变得越来越模糊。这种边界模糊性,虽然使数据集的平衡性得到了改善,但有时也会加大了分类算法进行分类的难度．

针对早期SMOTE算法的诸多不足,后期也提出了很多新的算法,对其进行了很多的改进,例如著名的Borderline SMOTE算法,感兴趣可以继续阅读相关论文.



## 2.3  欠采样

欠采样和过采样是相对的,过采样就是对少的样本进行采样生成更多的样本,那么欠采样就是对多的样本进行采样,降低类别多的样本数,使得我们的样本个数更少。

欠采样的好处非常多,例如当模型的训练对于内存和速度有较大的要求时，例如最近非常火的AutoML算法竞赛中,对于内存和时间的要求就比较高,此时欠采样的作用就会比较明显。在去年NIPS的AutoML比赛中,第三名的队伍通过多次对负样本进行欠采样然后进行最终算法的集成,达到的效果要比采用所有数据进行调参得到的结果还要好,欠采样可以:

- 带来训练数据的差异性,方便模型集成;
- 降低训练数据集的大小,加速模型的训练&降低内存的使用.
- 在某些特定的数据集上达到奇效。

在sklearn中,还有一种欠采样,叫做Tomek links欠采样,它的思路可以参考下图

![img](https://mmbiz.qpic.cn/mmbiz_png/k20ecIowF5ZIpca2l6vMH1qmq4UPMI4R2KfyZkKNu48tj1srGK5RLmRJPQXz4aibaLica9bCOp6oU4GIIiaUsicKZg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**如果有两个不同类别的样本，它们的最近邻都是对方，也就是A的最近邻是B，B的最近邻是A，那么A,B就是Tomek link**。我们要做的就是将所有Tomek link都删除掉。那么一个删除Tomek link的方法就是，将组成Tomek link的两个样本，如果有一个属于多数类样本，就将该多数类样本删除掉。这样我们可以发现正负样本就分得更开了。



# 3  小结

本篇文章我们给出了当碰到类别不平衡的问题时可以采用的三种常见方法,在我之前碰到的问题中,采用上面三种方案几乎都可以取得类似的效果,有时好一些,有时稍差一些,但是对最终的结果进行简单的集成,我们便可以在验证集和测试集上都取得不错的提升,大家也可以亲测,绝对有效。