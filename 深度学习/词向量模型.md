链接  <https://blog.csdn.net/xiayto/article/details/84730009>

# Word2Vec

### CBOW

之前说了那么多，现在我们正式开始接触word2vec中涉及到的两个模型，CBOW模型(Continuous Bag-Of-Words Model)和Skip-gram模型(Continuous Skip-gram Model)。CBOW是已知当前词的上下文，来预测当前词，而Skip-gram则相反，是在已知当前词的情况下，预测其上下文。二者的模型结构如下图所示：

![img](http://upload-images.jianshu.io/upload_images/4155986-b178fa035a6875b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

对于上面提到的两个模型，word2vec分别给出了两套框架，分别基于Hierarchical Softmax 和 Negative Sampling来进行设计，接下来，我们会分别对这两种CBOW模型进行讲解。

## 1、基于Hierarchical Softmax的CBOW模型

### 1.1 模型说明

之前我们提到过，基于神经网络的语言模型的目标函数通常取为如下的对数似然函数：

![img](http://upload-images.jianshu.io/upload_images/4155986-ee3c968681543178.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/704/format/webp)

其中的关键是条件概率p(w|Context(w))的构造。基于Hierarchical Softmax的CBOW模型优化的目标函数也形如上面的样子。
首先，我们来看一下CBOW的网络结构，它包括三层，分别为输入层，投影层和输出层，假设Context(w)是由词w的前后各c个词组成，下面的图对这三层做了相应的说明：

![img](http://upload-images.jianshu.io/upload_images/4155986-82f9d5d986060856.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

2）从有无隐藏层来看，神经概率模型有隐藏层，而CBOW没有隐藏层
3）从输出层来看，神经概率模型的输出层是线性结构，而CBOW是树形结构
所以，针对神经概率模型大规模的矩阵运算和softmax归一运算，CBOW对其作出了优化，首先去掉了隐藏层，同时输出层改用Huffman树，从而为利用Hierarchical Softmax技术奠定了基础。

### 1.2 梯度计算

Hierarchical Softmax技术是word2vec中用于提高性能的一项关键技术，为描述方便起见，在具体介绍这个技术之前，先引入若干相关记号，考虑Huffman树中的某个叶子结点，假设它对应词典D中的词w，记：

![img](http://upload-images.jianshu.io/upload_images/4155986-8d234a8fbe94e3ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

引入了这么多符号，我们举一个例子来说明上面这些符号，考虑我们在第一讲中提到了巴西世界杯的例子，它构造的Huffman树如下图所示：

![img](http://upload-images.jianshu.io/upload_images/4155986-93776177eaaa3d45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-a4d82dfd817f818c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

好了，定义了一大堆符号，我们接下来该考虑如何定义条件概率p(w|Context(w))呢？
还是考虑上面的巴西世界杯的例子，从根结点到足球这一叶子结点的过程中，我们经历了四次分支，每次分支相当于做了一次二分类。既然是从二分类的角度来考虑问题，那么对于每一个非叶子结点，就需要为其左右孩子结点指定一个类别，即哪个是正类（标签为1），哪个是负类（标签为0）。碰巧，除根结点外，树中每个结点都对应了一个取值为0或1的Huffman编码。因此在word2vec中，将编码为0的结点定义为负累，编码为0的点定义为正类，即约定：

![img](http://upload-images.jianshu.io/upload_images/4155986-fb71a239e724d686.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/816/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-1ca8db1acafedbdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/514/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-04677de37fab70a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/336/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-c9d21ceec2ec19e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

所以我们要求p(足球|Context(足球))，它跟上面这四个概率的关系就是：

![img](http://upload-images.jianshu.io/upload_images/4155986-ca0580009b657df4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/994/format/webp)

通过对足球这个例子的讲解，Hierarchical Softmax的基本思想就已经介绍完了，总结一下，对于词典D的任意词w，Huffman 树中必定存在一条从根结点到该词的路径，路径长度为l，则路径上存在l-1个分支，将每一个分支作为二分类，每一次分类产生一个概率，将所有的概率相乘，就得到所需的p(w|Context(w))。
条件概率p(w|Context(w))的一般公式可以写作：

![img](http://upload-images.jianshu.io/upload_images/4155986-e0418ea142b30535.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/868/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-f8d1e359e2bbd629.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

写成整体表达式就是：

![img](http://upload-images.jianshu.io/upload_images/4155986-8f728362902e9056.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-ca258e0697813c0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-232da3e83e19c024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-d5ef0d6b5e797ab4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/168/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-af491967a8045d1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/914/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-aac1d2a18066fada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

中间设计的推导过程如下：

![img](http://upload-images.jianshu.io/upload_images/4155986-e41c7b7ce4159d29.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-b308e1dbc2775f0c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

## 2、基于Negative Sampling的CBOW模型

可以看到，基于Hierarchical Softmax的CBOW模型采用了复杂的Huffman树，为了简化这一过程，又提出了基于Negative Sampling的CBOW模型，利用随机负彩样，大幅提升了计算性能。不过，其基本的计算思想仍是一样的。
在CBOW模型中，已知词w的上下文Context(w)，需要预测w，因此对于给定的Context(w)来说，词w就是一个正样本，其他词就是一个负样本了。关于负样本的选取，我们将在最后一节中进行介绍。假定现在已经选好了一个关于w的负样本字节NEG(w)，且对于D中的每一个词，定义：

![img](http://upload-images.jianshu.io/upload_images/4155986-0524820ea1ad4ed9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/654/format/webp)

即正样本的标签是1，负样本的标签是0。
所以，对于一个给定的(Context(w),w),我们希望最大化：

![img](http://upload-images.jianshu.io/upload_images/4155986-80c595a4c40cacd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/872/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-f22c050b59710f7c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-9d1e3d8c8460f0b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

由于只有一个正样本，所以g(w)又可以写为：

![img](http://upload-images.jianshu.io/upload_images/4155986-69fc493b1843301e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/930/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-c4cdf55a74b3c08c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-b2302eba946790ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-e179d80e035b7a11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/796/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-e3c0643786113376.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/776/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-f05a6d55a852e01c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-03fc2c3e4c5eb9c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

### 其他对CBOW的理解

<https://blog.csdn.net/u012762419/article/details/79366052>

### Skip-gram

之前说了那么多，现在我们正式开始接触word2vec中涉及到的两个模型，CBOW模型(Continuous Bag-Of-Words Model)和Skip-gram模型(Continuous Skip-gram Model)。CBOW是已知当前词的上下文，来预测当前词，而Skip-gram则相反，是在已知当前词的情况下，预测其上下文。二者的模型结构如下图所示：

![img](http://upload-images.jianshu.io/upload_images/4155986-b178fa035a6875b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

对于上面提到的两个模型，word2vec分别给出了两套框架，分别基于Hierarchical Softmax 和 Negative Sampling来进行设计，接下来，我们会分别对这两种Skip-gram模型进行讲解。

## 1、基于Hierarchical Softmax的Skip-gram模型

### 1.1 模型说明

之前我们提到过，基于神经网络的语言模型的目标函数通常取为如下的对数似然函数：

![img](http://upload-images.jianshu.io/upload_images/4155986-ee3c968681543178.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/704/format/webp)

其中的关键是条件概率p(w|Context(w))的构造。基于Hierarchical Softmax的CBOW模型优化的目标函数也形如上面的样子。那么对于Skip-gram模型来说，优化的目标函数变为：

![img](http://upload-images.jianshu.io/upload_images/4155986-7b2edb66564371a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/662/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-2de4e74018ab4ac7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

### 1.2 梯度计算

Skip-gram模型的重点是条件概率p(Context(w)|w)的构造，模型中将其定义为：

![img](http://upload-images.jianshu.io/upload_images/4155986-2b19daa21bca6cd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/914/format/webp)

按照Hierarchical Softmax的思想，p(u|v)可以写为：

![img](http://upload-images.jianshu.io/upload_images/4155986-d00623a69b9d47d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/694/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-f6577b5cc93d2e90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

所以对数似然函数的具体表达式为：

![img](http://upload-images.jianshu.io/upload_images/4155986-487080d028e10d4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

可以看到，这跟CBOW的形式十分类似，我们同样用随机梯度上升的方法，为求导方便起见，我们将三重求和符号里面的内容取出：

![img](http://upload-images.jianshu.io/upload_images/4155986-4556f7ad09ba4631.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

则参数的更新形式为：

![img](http://upload-images.jianshu.io/upload_images/4155986-ea8fa1695a543325.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-94a9fd7c98fddc6f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-a3105fcb8ebce42b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

但在实际的word2vec模型中，并不是等Context(w)中的所有词处理完之后才更新v(w)，而是每处理完一个词u，就刷新一次v(w)，具体为：

![img](http://upload-images.jianshu.io/upload_images/4155986-d2d89ffa89f7f338.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/994/format/webp)

## 2、基于Negative Sampling的Skip-gram模型

word2vec中基于Negative Sampling的Skip-gram模型，本质上和基于Negative Sampling的CBOW模型一样，这个地方看得我头昏眼花，不过一旦想明白，你肯定也能豁然开朗。
对于一个给定的样本(w,Context(w))，我们希望最大化：

![img](http://upload-images.jianshu.io/upload_images/4155986-a246ae5eeaf1210b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/914/format/webp)

看到了么，如果除去最外层的连乘符号，是不是就跟CBOW模型一样了，对的，word2vec模型就是将给定样本中Context(w)中的每一个词w1，分别与w进行组合(w,w1),这么一来，可以反过来把w1看成上下文，w作为要预测的词，这不就跟CBOW的思路一样了么？对的，就是这么神奇。
继续说上面的式子，其中：

![img](http://upload-images.jianshu.io/upload_images/4155986-97862516b28f0726.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-86e877f16cec7dd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-8f2faad0173cb260.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/912/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-004015a250f92d83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/916/format/webp)

![img](http://upload-images.jianshu.io/upload_images/4155986-45a81cac62cafad7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

那么，基于Negative Sampling的Skip-gram模型的伪代码如下图所示：

![img](https://upload-images.jianshu.io/upload_images/4155986-ee5b03116a8b41c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp)

