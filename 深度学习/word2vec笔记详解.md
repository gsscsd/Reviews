## word2vec数学原理详解

（一）[目录和前言](http://blog.csdn.net/itplus/article/details/37969519)

（二）[预备知识](http://blog.csdn.net/itplus/article/details/37969635)

（三）[背景知识](http://blog.csdn.net/itplus/article/details/37969817)

（四）[基于 Hierarchical Softmax 的模型](http://blog.csdn.net/itplus/article/details/37969979)

（五）[基于 Negative Sampling 的模型](http://blog.csdn.net/itplus/article/details/37998797)

（六）[若干源码细节](http://blog.csdn.net/itplus/article/details/37999613)

### 关于word2vec的相关答疑

Q1. gensim 和 google的 word2vec 里面并没有用到onehot encoder，而是初始化的时候直接为每个词随机生成一个N维的向量，并且把这个N维向量作为模型参数学习；所以word2vec结构中不存在文章图中显示的将V维映射到N维的隐藏层。

A1. 其实，本质是一样的，加上 one-hot encoder 层，是为了方便理解，因为这里的 N 维随机向量，就可以理解为是 V 维 one-hot encoder 输入层到 N 维隐层的权重，或者说隐层的输出（因为隐层是线性的）。每个 one-hot encoder 里值是 1 的那个位置，对应的 V 个权重被激活，其实就是『从一个V*N的随机词向量矩阵里，抽取某一行』。学习 N 维向量的过程，也就是优化 one-hot encoder 层到隐含层权重的过程

Q2. hierarchical softmax 获取词向量的方式和原先的其实基本完全不一样，我初始化输入的也不是一个onehot，同时我是直接通过优化输入向量的形式来获取词向量？如果用了hierarchical 结构我应该就没有输出向量了吧？

A2. 初始化输入依然可以理解为是 one-hot，同上面的回答；确实是只能优化输入向量，没有输出向量了。具体原因，我们可以梳理一下不用 hierarchical (即原始的 softmax) 的情形：

> 隐含层输出一个 N 维向量 x, 每个x 被一个 N 维权重 w 连接到输出节点上，有 V 个这样的输出节点，就有 V 个权重 w，再套用 softmax 的公式，变成 V 分类问题。这里的类别就是词表里的 V 个词，所以一个词就对应了一个权重 w，从而可以用 w 作为该词的词向量，即文中的输出词向量。
> PS. 这里的 softmax 其实多了一个『自由度』，因为 V 分类只需要 V-1 个权重即可

我们再看看 hierarchical softmax 的情形：

> 隐含层输出一个 N 维向量 x, 但这里要预测的目标输出词，不再是用 one-hot 形式表示，而是用 huffman tree 的编码，所以跟上面 V 个权重同时存在的原始 softmax 不一样， 这里 x 可以理解为先接一个输出节点，即只有一个权重 w1 ，输出节点输出 1/1+exp(-w*x)，变成一个二分类的 LR，输出一个概率值 P1，然后根据目标词的 huffman tree 编码，将 x 再输出到下一个 LR，对应权重 w2，输出 P2，总共遇到的 LR 个数（或者说权重个数）跟 huffman tree 编码长度一致，大概有 log(V) 个，最后将这 log(V) 个 P 相乘，得到属于目标词的概率。但注意因为只有 log(V) 个权重 w 了，所以跟 V 个词并不是一一对应关系，就不能用 w 表征某个词，从而失去了词向量的意义
> PS. 但我个人理解，这 log(V) 个权重的组合，可以表示某一个词。因为 huffman tree 寻找叶子节点的时候，可以理解成是一个不断『二分』的过程，不断二分到只剩一个词为止。而每一次二分，都有一个 LR 权重，这个权重可以表征该类词，所以这些权重拼接在一起，就表示了『二分』这个过程，以及最后分到的这个词的『输出词向量』。
> 我举个例子：
> 假设现在总共有 (A,B,C)三个词，huffman tree 这么构建：
> 第一次二分： (A,B), (C)
> 假如我们用的 LR 是二分类 softmax 的情形（比常见 LR 多了一个自由度），这样 LR 就有俩权重，权重 w1_1 是属于 (A,B) 这一类的，w1_2 是属于 (C) 的, 而 C 已经到最后一个了，所以 C 可以表示为 w1_2
> 第二次二分： (A), (B)
> 假设权重分别对应 w2_1 和 w2_2，那么 A 就可以表示为 [w1_1, w2_1], B 可以表示为 [w1_1, w2_2]
> 这样， A,B,C 每个词都有了一个唯一表示的词向量（此时他们长度不一样，不过可以用 padding 的思路，即在最后补0）
> 当然了，一般没人这么干。。。开个脑洞而已

Q3. 是否一定要用Huffman tree?

A3. 未必，比如用完全二叉树也能达到O(log(N))复杂度。但 Huffman tree 被证明是更高效、更节省内存的编码形式，所以相应的权重更新寻优也更快。 举个简单例子，高频词在Huffman tree中的节点深度比完全二叉树更浅，比如在Huffman tree中深度为3，完全二叉树中深度为5，则更新权重时，Huffman tree只需更新3个w，而完全二叉树要更新5个，当高频词频率很高时，算法效率高下立判.

Word2vec训练方面采用的HSoftmax以及负采样确实可以认为是创新不大。但Word2vec流行的主要原因也不在于此。主要原因在于以下3点：

1. 极快的训练速度。以前的语言模型优化的目标是MLE，只能说词向量是其副产品。Mikolov应该是第一个提出抛弃MLE（和困惑度）指标，就是要学习一个好的词嵌入。如果不追求MLE，模型就可以大幅简化，去除隐藏层。再利用HSoftmax以及负采样的加速方法，可以使得训练在小时级别完成。而原来的语言模型可能需要几周时间。

2. 一个很酷炫的man-woman=king-queen的示例。这个示例使得人们发现词嵌入还可以这么玩，并促使词嵌入学习成为了一个研究方向，而不再仅仅是神经网络中的一些参数。

3. word2vec里有大量的tricks，比如噪声分布如何选？如何采样？如何负采样？等等。这些tricks虽然摆不上台面，但是对于得到一个好的词向量至关重要。

   举一个生活中的例子，语言模型和word2vec的关系可以类比于单反相机和美颜手机，它们的受众不一样。就照片质量（MLE）而言，单反肯定好。但如果更关心美颜（词嵌入）和便携性（训练速度），美颜手机就更受欢迎。