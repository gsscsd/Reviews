# 深度学习—BN的理解（一）

### **0、问题**

　　**机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的**，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？**BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。**

　　思考一个问题：为什么传统的神经网络在训练开始之前，要对输入的数据做Normalization?原因在于神经网络学习过程**本质上是为了学习数据的分布**，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另一方面，一旦在mini-batch梯度下降训练的时候，每批训练数据的分布不相同，那么网络就要在每次迭代的时候去学习以适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对所有训练数据做一个Normalization预处理的原因。

　　为什么深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢？这是个在DL领域很接近本质的好问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，**BN本质上也是解释并从某个不同的角度来解决这个问题的。**

### **1、“Internal Covariate Shift”问题**

　　从论文名字可以看出，BN是用来解决“**Internal Covariate Shift**”问题的，那么首先得理解什么是“Internal Covariate Shift”？

　　论文首先说明Mini-Batch SGD相对于One Example SGD的两个优势：**梯度更新方向更准确；并行计算速度快；**（为什么要说这些？因为BatchNorm是基于Mini-Batch SGD的，所以先夸下Mini-Batch SGD，当然也是大实话）；然后吐槽下SGD训练的缺点：**超参数调起来很麻烦**。（作者隐含意思是用BN就能解决很多SGD的缺点）

　　接着**引入covariate shift的概念**：如果ML系统实例集合<X,Y>中的输入值X的分布老是变，这不符合IID假设，网络模型很难稳定的学规律，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是**在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。**

　　然后提出了BatchNorm的基本思想**：能不能让每个隐层节点的激活输入分布固定下来呢**？这样就避免了“Internal Covariate Shift”问题了，顺带解决反向传播中梯度消失问题。BN 其实就是在做 feature scaling，而且它的目的也是为了在训练的时候避免这种 Internal Covariate Shift 的问题，只是刚好也解决了 sigmoid 函数梯度消失的问题。

　　BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓**白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛**，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实**深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已**，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，**可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。**

**Internal Covariate Shift 造成的问题**

**简而言之，每个神经元的输入数据不再是"独立同分布"。**

**其一，靠近输出层的隐含层网络的参数需要不断适应新的输入数据分布，降低学习速度。**

**其二，靠近输入层的隐含层网络的参数变化可能趋向于变大或者变小，导致后面的网络层落入激活函数的饱和区，使得学习过早停止。**

**其三，每层的参数更新都会影响到其他层，因此每层的参数更新策略需要尽可能的谨慎。**

### **2、BatchNorm的本质思想**

　　BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，**之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因**，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实**就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。**

　　THAT’S IT。其实一句话就是：**对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题**。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。BN说到底就是这么个机制，方法很简单，道理很深刻。

上面说得还是显得抽象，下面更形象地表达下这种调整到底代表什么含义。

![img](https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405225246905-37854887.png)

　　假设某个隐层神经元原先的激活输入x取值符合正态分布，正态分布均值是-2，方差是0.5，对应上图中最左端的浅蓝色曲线，通过BN后转换为均值为0，方差是1的正态分布（对应上图中的深蓝色图形），意味着什么，意味着输入x的取值正态分布整体右移2（均值的变化），图形曲线更平缓了（方差增大的变化）。这个图的意思是，BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差为1的正态分布通过平移均值压缩或者扩大曲线尖锐程度，调整为均值为0方差为1的正态分布。

　　那么把激活输入x调整到这个正态分布有什么用？首先我们看下均值为0，方差为1的标准正态分布代表什么含义：

![img](https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405225314624-527885612.png)

　　这意味着在一个标准差范围内，也就是说64%的概率x其值落在[-1,1]的范围内，在两个标准差范围内，也就是说95%的概率x其值落在了[-2,2]的范围内。那么这又意味着什么？我们知道，激活值x=WU+B,U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid，那么看下sigmoid(x)其图形：

![img](https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407143109455-1460017374.png)

图3. Sigmoid(x)

及sigmoid(x)的导数为：G’=f(x)*(1-f(x))，因为f(x)=sigmoid(x)在0到1之间，所以G’在0到0.25之间，其对应的图如下：

![img](https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180407142351924-124461667.png)

　　假设没有经过BN调整前x的原先正态分布均值是-6，方差是1，那么意味着95%的值落在了[-8,-4]之间，那么对应的Sigmoid（x）函数的值明显接近于0，这是典型的梯度饱和区，在这个区域里梯度变化很慢，为什么是梯度饱和区？请看下sigmoid(x)如果取值接近0或者接近于1的时候对应导数函数取值，接近于0，意味着梯度变化很小甚至消失。而假设经过BN后，均值是0，方差是1，那么意味着95%的x值落在了[-2,2]区间内，很明显这一段是sigmoid(x)函数接近于线性变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也即是梯度变化较大，对应导数函数图中明显大于0的区域，就是梯度非饱和区。

　　从上面几个图应该看出来BN在干什么了吧？其实就是把隐层神经元激活输入x=WU+B从变化不拘一格的正态分布通过BN操作拉回到了均值为0，方差为1的正态分布，即原始正态分布中心左移或者右移到以0为均值，拉伸或者缩减形态形成以1为方差的图形。什么意思？**就是说经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。**

　　但是很明显，看到这里，稍微了解神经网络的读者一般会提出一个疑问：如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。所以BN**为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale\*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。**

　　**核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。**当然，这是我的理解，论文作者并未明确这样说。但是很明显这里的scale和shift操作是会有争议的，因为按照论文作者论文里写的理想状态，就会又通过scale和shift操作把变换后的x调整回未变换的状态，那不是饶了一圈又绕回去原始的“Internal Covariate Shift”问题里去了吗，感觉论文作者并未能够清楚地解释scale和shift操作的理论原因。

### **3、训练阶段如何做BatchNorm**

![img](https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200545466-1938722586.png)

　　对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，其具体BN操作就是对于隐层内每个神经元的激活值来说，进行如下变换：

![img](https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200638763-60126435.png)

　　要注意，这里t层某个神经元的x(k)不是指原始输入，就是说不是t-1层每个神经元的输出，而是t层这个神经元的线性激活x=WU+B，这里的U才是t-1层神经元的输出。变换的意思是：某个神经元对应的原始的激活x通过减去mini-Batch内m个实例获得的m个激活x求得的均值E(x)并除以求得的方差Var(x)来进行转换。

**其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办?变换重构，引入了可学习参数γ、β，这就是算法关键之处*

![img](https://img-blog.csdn.net/20160312190113493)

每一个神经元xk都会有一对这样的参数γ、β。这样其实当：

![img](https://img-blog.csdn.net/20160312190323411)、![img](https://img-blog.csdn.net/20160312190336072)

是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。

　　上文说过**经过这个变换后某个神经元的激活x形成了均值为0，方差为1的正态分布，目的是把值往后续要进行的非线性变换的线性区拉动，增大导数值，增强反向传播信息流动性，加快训练收敛速度。**但是这样会导致网络表达能力下降，为了防止这一点，**每个神经元增加两个调节参数（scale和shift），这两个参数是通过训练来学习到的，用来对变换后的激活反变换，使得网络表达能力增强，即对变换后的激活进行如下的scale和shift操作**，这其实是变换的反操作：

![img](https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200847123-286502102.png)

　　BN其具体操作流程，如论文中描述的一样：

![img](https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012200911290-1256604003.png)

　　走一遍Batch Normalization网络层的前向传播过程。

![img](https://img2018.cnblogs.com/blog/1393464/201810/1393464-20181012204510101-2110717569.jpg)

### **4、BatchNorm的推理(Inference)过程**

　　BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，很明显输入就只有一个实例，看不到Mini-Batch其它实例，那么这时候怎么对输入做BN呢？因为很明显一个实例是没法算实例集合求出的均值和方差的。这可如何是好？既然没有从Mini-Batch数据里可以得到的统计量，那就想其它办法来获得这个统计量，就是均值和方差。**可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量**，因为本来就打算用全局的统计量，只是因为计算量等太大所以才会用Mini-Batch这种简化方式的，那么在推理的时候直接用全局统计量即可。

　　决定了获得统计量的数据范围，那么接下来的问题是如何获得均值和方差的问题。很简单，因为每次做Mini-Batch训练时，都会有那个Mini-Batch里m个训练实例获得的均值和方差，现在要全局统计量，只要把每个Mini-Batch的均值和方差统计量记住，然后对这些均值和方差求其对应的数学期望即可得出全局统计量

#### 4.1 Batch Normalization用在网络的哪里？

根据论文中说，BN可以应用于一个神经网络的任何神经元上。论文中指出把**BN置于网络激活函数的前面**。

#### 4.2 BN算法在CNN中的应用

**BN算法**在**CNN**中往往放在每个**卷积层之后，ReLU操作之前**。在CNN中使用BN是把每个特征图看做一个神经元，计算该特征图对应数据的均值和方差进行归一化，并且每个特征图对应两个学习变量γ、β。想具体了解BN在CNN中的使用方法，可以详细看一下[深度学习（二十九）Batch Normalization 学习笔记 - CSDN博客](https://link.zhihu.com/?target=https%3A//blog.csdn.net/hjimce/article/details/50866313)这篇文章。

#### 4.3 数据输入前的预处理

说到神经网络输入数据预处理，最好的算法莫过于白化预处理。然而白化计算量太大了，很不划算，还有就是白化不是处处可微的，所以在深度学习中，其实很少用到白化。**经过白化预处理后，数据满足条件：a、特征之间的相关性降低，这个就相当于PCA；b、数据均值、标准差归一化，也就是使得每一维特征均值为0，标准差为1**。如果数据特征维数比较大，要进行PCA，也就是实现白化的第1个要求，是需要计算特征向量，计算量非常大，于是为了简化计算，作者忽略了第1个要求，仅仅使用了下面的公式进行预处理，也就是近似白化预处理：

![img](https://img-blog.csdn.net/20160312181715397)

公式简单粗糙，但是依旧很牛逼。因此后面我们也将用这个公式，对某一个层网络的输入数据做一个归一化处理。需要注意的是，我们训练过程中采用batch 随机梯度下降，上面的E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。

**5、BatchNorm的好处**

（1）**可以使用更高的学习率，BN有快速收敛的特性。**在没有加Batch Normalization的网络中我们要慢慢的调整学习率时，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适。现在，我们在网络中加入Batch Normalization时，可以采用初始化很大的学习率，然后学习率衰减速度也很大，因此这个算法收敛很快。

（2）**模型中BN可以代替dropout或者使用较低的dropout。**dropout是经常用来防止过拟合的方法，但是模型中加入BN减少dropout，可以大大提高模型训练速度，提高网络泛化性能。

（3）**减少L2权重衰减系数。**用了**Batch Normalization**后，可以把L2权重衰减系数降低，论文中降低为原来的5倍。

（4）**取消Loacl Response Normalization层。**（局部响应归一化是Alexnet网络用到的方法），因为BN本身就是一个归一化网络层。

（5）**BN本质上解决了反向传播过程中梯度消失的问题。**BN算法对Sigmoid激活函数的提升非常明显，解决了困扰学术界十几年的sigmoid过饱和造成梯度消失的问题。在深度神经网络中，靠近输入的网络层在梯度下降的时候，得到梯度值太小，导致深层神经网络只有靠近输出层的那几层网络在学习。因为数据使用BN后，归一化的数据仅使用了sigmoid线性的部分。

（6）**可以把训练数据彻底打乱。**防止了每批训练的时候，某一个样本经常被挑选到。论文中指出这个操作可以提高1%的精度。

> 自从Google在2015年提出Batch Normalization方法优化深度神经网络之后，BN就成为深度学习中重要的优化方法。并且，自Batch Normalization提出之后，又有许多基于BN改进的优化方法：Layer Normalization、Weight Normalization、Cosine Normalization。这些优化深度神经网络模型训练的方法，也成为面试中经常被问到的题目。