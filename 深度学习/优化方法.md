## 常见的优化方法

### **1.SGD&BGD&Mini-BGD**:

**SGD**(stochastic gradient descent):随机梯度下降,算法在每读入一个数据都会立刻计算loss function的梯度来update参数．假设loss function为L(w)，

$$w-=\eta \bigtriangledown_{w_{i}}L(w_{i})$$

- Pros:收敛的速度快；可以实现在线更新；能够跳出局部最优
- Cons:很容易陷入到局部最优，困在马鞍点．

**BGD**(batch gradient descent):批量梯度下降，算法在读取整个数据集后累加来计算损失函数的的梯度.

$$w-=\eta \bigtriangledown_{w_{i}}L(w)$$

- Pros:如果loss function为convex，则基本可以找到全局最优解
- Cons:数据处理量大，导致梯度下降慢;不能实时增加实例，在线更新；训练占内存

**Mini-BGD**(mini-batch gradient descent):顾名思义，选择小批量数据进行梯度下降，这是一个折中的方法．采用训练集的子集(mini-batch)来计算loss function的梯度．

$$w-=\eta \bigtriangledown_{w_{i:i+n}}L(w_{i:i+n})$$

这个优化方法用的也是比较多的，计算效率高而且收敛稳定，是现在深度学习的主流方法．

上面的方法都存在一个问题，就是update更新的方向完全依赖于计算出来的梯度．很容易陷入局部最优的马鞍点．能不能改变其走向，又保证原来的梯度方向．就像向量变换一样，我们模拟物理中物体流动的动量概念(惯性).引入Momentum的概念．

### **2.Momentum**

在更新方向的时候保留之前的方向，增加稳定性而且还有摆脱局部最优的能力

若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。一种形象的解释是：我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，ηη可视为空气阻力，若球的方向发生变化，则动量会衰减。

**Adagrad**：(adaptive gradient)自适应梯度算法,是一种改进的随机梯度下降算法．
以前的算法中，每一个参数都使用相同的学习率α. Adagrad算法能够在训练中自动对learning_rate进行调整，出现频率较低参数采用较大的α更新．出现频率较高的参数采用较小的α更新．根据描述这个优化方法很适合处理稀疏数据．

$$G=\sum ^{t}_{\tau=1}g_{\tau} g_{\tau}^{T} 　其中 s.t. g_{\tau}=\bigtriangledown L(w_{i})\\ w_{j}=w_{j}-\frac{\eta}{\sqrt{G_{j,j}}}g_{j}$$

**RMSprop**:(root mean square propagation)也是一种自适应学习率方法．不同之处在于，Adagrad会累加之前所有的梯度平方，RMProp仅仅是计算对应的平均值．可以缓解Adagrad算法学习率下降较快的问题．

$$ v(w,t)=\gamma v(w,t-1)+(1-\gamma)(\bigtriangledown L(w_{i}))^{2}  ,其中 \gamma 是遗忘因子 \\ w=w-\frac{\eta}{\sqrt{v(w,t)}}\bigtriangledown L(w_{i}) $$

**Adam**:(adaptive moment estimation)是对RMSProp优化器的更新.利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率.
优点:每一次迭代学习率都有一个明确的范围,使得参数变化很平稳. **Adam是实际学习中最常用的算法**

$$m_{w}^{t+1}=\beta_{1}m_{w}^{t}+(1-\beta_{1}) \bigtriangledown L^{t} ,m为一阶矩估计 \\ v_{w}^{t+1}=\beta_{2}m_{w}^{t}+(1-\beta_{2}) (\bigtriangledown L^{t})^{2},v为二阶矩估计 \\ \hat{m}_{w}=\frac{m_{w}^{t+1}}{1-\beta_{1}^{t+1}}，估计校正，实现无偏估计 \\ \hat{v}_{w}=\frac{v_{w}^{t+1}}{1-\beta_{2}^{t+1}} \\  w^{t+1} \leftarrow=w^{t}-\eta \frac{\hat{m}_{w}}{\sqrt{\hat{v}_{w}}+\epsilon} $$

![](https://img2018.cnblogs.com/blog/1425630/201809/1425630-20180917092038787-2101213597.gif)

### 一句话总结所有的方法的优缺点

### SGD

没什么好说的

#### Momentum

如其名， 动量， 把历史改变作为动量累加到当前梯度上。**动量+梯度**

#### Nesterov

也是动量， 先动量， 根据动量后的值计算梯度更新。
先动量， 求跳跃后的梯度

#### Adagrad

用所有梯度平方和后开根号除当前梯度， 来反向调节更新

防止更新率过大的时候， 防止震荡；更新率过小的时候， 让梯度加速， 当然也看的出来很多问题， 至少不该使用所有的以前的梯度， 这样， 越训练到后面， 越慢， 到最后， 受到以前梯度平方的影响， 更新就非常慢了

> adapt + gradient descent
> 适合处理稀疏梯度

#### Adadelta

以前的梯度平方衰减后累加开根号后来除现在的梯度

证明了我对adagrad的理解， 无衰减累加梯度**2会过多的影响现在的取值， 加上衰减后， 以前的梯度的影响会渐渐消失， 越近的梯度对现在的影响越大。

> Adagrad delta版本
> 现在的默认参数， 估计针对是归一化数据的， 所以也证明了， 对数据归一化非常重要， 不然咋个以前的梯度都较大的话， 现在还是训练越来越慢。

- 训练初中期，加速效果不错，很快
- 训练后期，反复在局部最小值附近抖动

#### RMSprop

Adadelta的变体

`其实没咋个看懂变体后公式中E的含义， 猜测主要简化了计算， 并且只计算最近一次的梯度变化么？`

> RMS均方根的意思

- RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
- 适合处理非平稳目标 - 对于RNN效果很好

#### Adam

带有动量项的RMSprop

- 适用于大数据集和高维空间

> Ada + Momentum

#### Adamax

带有学习率上限的Adam

#### Nadam

带有Nesterov动量项的Adam

- 一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果

> Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。
> 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果